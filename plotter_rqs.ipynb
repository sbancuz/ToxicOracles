{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Oracles Plotter\n",
    "This notebook provides the tools to create the plots, showing the results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# imports\n",
    "import os\n",
    "# from generalComparison import load_data, grouped\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.inter_rater as st\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from bisect import bisect_left\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.inter_rater as st\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import itertools as it\n",
    "from bisect import bisect_left\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "from pandas import Categorical\n",
    "from generalComparison import load_data, grouped\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# using the function load_data, the data is loaded in a pandas dataframe with the columns: iteration, score, system_generator, prompt_generator, delta_time_evaluation, delta_time_generation, delta_time_response, numberIterations\n",
    "results=[\"results/finalTests\", \"results/rebuttal\"]\n",
    "folders=[]\n",
    "# each config is a folder with the results of the experiments\n",
    "for results in results:\n",
    "    folders.extend([f.path for f in os.scandir(results) if f.is_dir()]) #and f.name != \"vicunaUC_vicunaUC\"])\n",
    "#folders = [f.path for f in os.scandir(results) if f.is_dir()] #and f.name != \"vicunaUC_vicunaUC\"]\n",
    "\n",
    "df=load_data(input=folders, criteria=\"max\", includeBaseline=True, max_samples=100)\n",
    "\n",
    "# renaming the values in file column \n",
    "df[\"file\"]=df[\"file\"].replace({\"max\":\"Vanilla\", \"max_fs\": \"IE\", \"max_fs_glit\": \"IE+GL\", \"max_mem_5_fs_glit\":\"IE+SE+GL\", \"baseline\": \"RS\", \"JailBreakPrompts-Mistral\": \"JailbreakPrompts\", \"JailbreakPrompts-vicunaUC\": \"JailbreakPrompts\", \"JailbreakPrompts-vicuna\": \"JailbreakPrompts\",\"JailbreakPrompts-llama3\": \"JailbreakPrompts\"})\n",
    "\n",
    "# pick only the last iteration (shown with numberIteration)\n",
    "dfFinal = df[df['iteration'] == df['numberIterations']]\n",
    "# drop the iteration column\n",
    "dfFinal = dfFinal.drop(columns=['numberIterations'])\n",
    "# list of systems under test\n",
    "suts=dfFinal[\"system_under_test\"].unique()\n",
    "print(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "baselines = ['RS', 'JailbreakPrompts', 'advbench', 'maliciousInstruct']\n",
    "evotox = ['Vanilla', 'IE', 'IE+GL', 'IE+SE+GL']\n",
    "\n",
    "plt.rcParams.update({'font.size': 19})\n",
    "for sut in suts:\n",
    "    # plots a boxplot of the scores for each prompt generator, by file name\n",
    "    prompt_generators=dfFinal[dfFinal[\"system_under_test\"]==sut][\"prompt_generator\"].unique()\n",
    "    # one subplot for each prompt generator\n",
    "    \n",
    "    bl_data = dfFinal[(dfFinal[\"system_under_test\"]==sut) & (dfFinal[\"file\"].isin(baselines))]\n",
    "    \n",
    "    for i, prompt_generator in enumerate(prompt_generators):\n",
    "        \n",
    "        data=dfFinal[(dfFinal[\"system_under_test\"]==sut) & (dfFinal[\"prompt_generator\"]==prompt_generator) & (dfFinal[\"file\"].isin(evotox))]\n",
    "        data = pd.concat([bl_data, data])\n",
    "        \n",
    "        #width=5*len(prompt_generators)\n",
    "        fig = plt.figure(figsize=(3.5, 5))\n",
    "        ax = plt.gca()\n",
    "\n",
    "        if len(prompt_generators)==1:\n",
    "            # only one prompt generator\n",
    "            #sns.boxplot(x=\"file\", y=\"score\", data=data, ax=ax)\n",
    "            # sns.boxplot(x=\"file\", y=\"score\", data=data, ax=ax, showmeans=True, hue=\"file\", palette=\"pastel\")\n",
    "            sns.boxplot(\n",
    "                x=\"file\", y=\"score\", data=data, ax=ax, showmeans=True, hue=\"file\",\n",
    "                meanprops={'marker':'^', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':'9'}\n",
    "            )\n",
    "            #ax.set_title(\"Prompt Generator: \"+prompt_generator)\n",
    "            ax.set_ylabel(\"Score\")\n",
    "            ax.set_xlabel(\"\")\n",
    "            # rotate the x labels\n",
    "            # ax.tick_params(axis='x', rotation=35)\n",
    "            ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=30, ha='right')\n",
    "            # ax.set_ylim(-0.05, 1.05)\n",
    "            ax.set_ylim(0, 1)\n",
    "        else:\n",
    "            # sns.boxplot(x=\"file\", y=\"score\", data=data, ax=ax, showmeans=True, hue=\"file\", palette=\"pastel\")\n",
    "            sns.boxplot(\n",
    "                x=\"file\", y=\"score\", data=data, ax=ax, showmeans=True, hue=\"file\",\n",
    "                meanprops={'marker':'^', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':'9'}\n",
    "            )\n",
    "            #ax.set_title(\"Prompt Generator: \"+prompt_generator)\n",
    "            ax.set_ylabel(\"Score\")\n",
    "            ax.set_xlabel(\"\")\n",
    "            # rotate the x labels\n",
    "            # ax.tick_params(axis='x', rotation=35)\n",
    "            ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=30, ha='right')\n",
    "            ax.set_xticks([])\n",
    "            # ax.set_ylim(-0.05, 1.05)\n",
    "            ax.set_ylim(0, 1)\n",
    "            \n",
    "        plt.savefig(\"./figures/rq1_{}_{}.pdf\".format(sut, prompt_generator), bbox_inches=\"tight\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def VD_A(treatment: List[float], control: List[float]):\n",
    "    m = len(treatment)\n",
    "    n = len(control)\n",
    "    #if m != n:\n",
    "    #    raise ValueError(\"Data d and f must have the same length\")\n",
    "    r = ss.rankdata(treatment + control)\n",
    "    r1 = sum(r[0:m])\n",
    "    # Compute the measure\n",
    "    # A = (r1/m - (m+1)/2)/n # formula (14) in Vargha and Delaney, 2000\n",
    "    A = (2 * r1 - m * (m + 1)) / (2 * n * m)  # equivalent formula to avoid accuracy errors\n",
    "    levels = [0.147, 0.33, 0.474]  # effect sizes from Hess and Kromrey, 2004\n",
    "    magnitude = [\"negligible\", \"small\", \"medium\", \"large\"]\n",
    "    scaled_A = (A - 0.5) * 2\n",
    "    magnitude = magnitude[bisect_left(levels, abs(scaled_A))]\n",
    "    estimate = A\n",
    "    return estimate, magnitude\n",
    "\n",
    "baselines = ['RS', 'JailbreakPrompts', 'advbench', 'maliciousInstruct']\n",
    "configurations = ['Vanilla', 'IE', 'IE+GL', 'IE+SE+GL']\n",
    "pairs = list(it.product(configurations, baselines))\n",
    "\n",
    "for sut in suts:\n",
    "    bl_data = dfFinal[(dfFinal[\"system_under_test\"]==sut) & (dfFinal[\"file\"].isin(baselines))]\n",
    "    \n",
    "    for i, prompt_generator in enumerate(prompt_generators):\n",
    "        \n",
    "        data=dfFinal[(dfFinal[\"system_under_test\"]==sut) & (dfFinal[\"prompt_generator\"]==prompt_generator) & (dfFinal[\"file\"].isin(evotox))]\n",
    "        data = pd.concat([bl_data, data])\n",
    "        \n",
    "        for pair in pairs:\n",
    "\n",
    "            data_1 = data[data[\"file\"]==pair[0]][\"score\"].to_list()\n",
    "            data_2 = data[data[\"file\"]==pair[1]][\"score\"].to_list()\n",
    "            \n",
    "            if len(data_1)==0 or len(data_2)==0:\n",
    "                continue\n",
    "            \n",
    "            stat, pvalue = st.stats.mannwhitneyu(data_1, data_2)\n",
    "            est, mag = VD_A(data_1, data_2)\n",
    "            \n",
    "            print(\"(SUT) {} - (PG) {}, {} vs. {} : p {:.1e} e {:.2f} \".format(sut, prompt_generator, pair[0], pair[1], pvalue, est))\n",
    "\n",
    "\n",
    "pairs = [(c1, c2) for i, c1 in enumerate(configurations) for c2 in configurations[i+1:]]\n",
    "\n",
    "for sut in suts:\n",
    "\n",
    "    prompt_generators=dfFinal[dfFinal[\"system_under_test\"]==sut][\"prompt_generator\"].unique()\n",
    "    \n",
    "    for i, prompt_generator in enumerate(prompt_generators):\n",
    "        \n",
    "        data=dfFinal[(dfFinal[\"system_under_test\"]==sut) & (dfFinal[\"prompt_generator\"]==prompt_generator) & (dfFinal[\"file\"].isin(evotox))]\n",
    "        \n",
    "        for pair in pairs:\n",
    "            \n",
    "            data_1 = data[data[\"file\"]==pair[0]][\"score\"].to_list()\n",
    "            data_2 = data[data[\"file\"]==pair[1]][\"score\"].to_list()\n",
    "            \n",
    "            if len(data_1)==0 or len(data_2)==0:\n",
    "                continue\n",
    "            \n",
    "            stat, pvalue = st.stats.mannwhitneyu(data_1, data_2)\n",
    "            est, mag = VD_A(data_1, data_2)\n",
    "            \n",
    "            print(\"(SUT) {} - (PG) {}, {} vs. {} : p {:.1e} e {:.2f} \".format(sut, prompt_generator, pair[0], pair[1], pvalue, est))\n",
    "            "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# define a new dataframe, such that:\n",
    "# - the columns are: prompt_generator, score\n",
    "# - prompt generator 'self': all the runs with system equal prompt generator, excluding vicuna and vicunaUC\n",
    "# - prompt generator 'Vicuna': all the runs with Vicuna as prompt generator (Mistral, LLama3)\n",
    "# - prompt generator 'VicunaUC': all the runs with VicunaUC as prompt generator (Mistral, LLama3)\n",
    "\n",
    "folders = [f.path for f in os.scandir(results) if f.is_dir()]\n",
    "df=load_data(input=folders, criteria=\"max\", includeBaseline=False, max_samples=100)\n",
    "\n",
    "# first, the data for the prompt generator 'self'\n",
    "dfSelf=df[(df[\"prompt_generator\"]==df[\"system_under_test\"]) & (df[\"prompt_generator\"]!=\"vicuna\") & (df[\"prompt_generator\"]!=\"vicunaUC\")].copy()\n",
    "# add the prompt_generator column as 'self'\n",
    "dfSelf[\"prompt_generator\"]=\"Self\"\n",
    "\n",
    "# second, the data for the prompt generator 'Vicuna'\n",
    "dfVicuna=df[(df[\"prompt_generator\"]==\"vicuna\")].copy()\n",
    "# add the prompt_generator column as 'Vicuna'\n",
    "dfVicuna[\"prompt_generator\"]=\"Vicuna\"\n",
    "\n",
    "# third, the data for the prompt generator 'VicunaUC'\n",
    "dfVicunaUC=df[(df[\"prompt_generator\"]==\"vicunaUC\")].copy()\n",
    "# add the prompt_generator column as 'VicunaUC'\n",
    "dfVicunaUC[\"prompt_generator\"]=\"VicunaUC\"\n",
    "\n",
    "# concatenate the three dataframes\n",
    "dfConcat=pd.concat([dfSelf, dfVicuna, dfVicunaUC])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dfLast=df[df[\"iteration\"]==df[\"numberIterations\"]]\n",
    "dfSelf=dfLast[(dfLast[\"prompt_generator\"]==dfLast[\"system_under_test\"]) & (dfLast[\"prompt_generator\"]!=\"vicuna\") & (dfLast[\"prompt_generator\"]!=\"vicunaUC\")].copy()\n",
    "dfSelf[\"prompt_generator\"]=\"Self\"\n",
    "dfVicuna=dfLast[(dfLast[\"prompt_generator\"]==\"vicuna\")].copy()\n",
    "dfVicuna[\"prompt_generator\"]=\"Vicuna\"\n",
    "dfVicunaUC=dfLast[(dfLast[\"prompt_generator\"]==\"vicunaUC\")].copy()\n",
    "dfVicunaUC[\"prompt_generator\"]=\"VicunaU\"\n",
    "dfConcat=pd.concat([dfSelf, dfVicuna, dfVicunaUC])\n",
    "# plot the average score\n",
    "plt.rcParams.update({'font.size': 19})\n",
    "fig = plt.figure(figsize=(3.5, 5))\n",
    "ax = plt.gca()\n",
    "# plot the boxplot\n",
    "# sns.boxplot(x=\"prompt_generator\", y=\"score\", ax=ax, data=dfConcat,  showmeans=True, hue=\"prompt_generator\", palette=\"pastel\")\n",
    "sns.boxplot(x=\"prompt_generator\", y=\"score\", ax=ax, data=dfConcat,  showmeans=True, hue=\"prompt_generator\", meanprops={'marker':'^', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':'9'})\n",
    "#plt.title(\"Prompt Generator Comparison\")\n",
    "plt.ylabel(\"Score\")\n",
    "ax.set_xlabel(\"\")\n",
    "# rotate the x labels\n",
    "# ax.tick_params(axis='x', rotation=35)\n",
    "ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=30, ha='right')\n",
    "# ax.set_ylim(-0.05, 1.05)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.savefig(\"./figures/rq1_pg_score.pdf\", bbox_inches=\"tight\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prompt_generators=dfConcat[\"prompt_generator\"].unique()\n",
    "\n",
    "scores: Dict[str, List[float]] = dict()\n",
    "\n",
    "for pg in prompt_generators:\n",
    "    data = dfConcat[dfConcat[\"prompt_generator\"]==pg][\"score\"].to_list()\n",
    "    \n",
    "    scores[pg] = data\n",
    "    \n",
    "pairs = [(c1, c2) for i, c1 in enumerate(prompt_generators) for c2 in prompt_generators[i+1:]]\n",
    "\n",
    "for pair in pairs:\n",
    "    data_1 = scores[pair[0]]\n",
    "    data_2 = scores[pair[1]]\n",
    "    \n",
    "    stat, pvalue = st.stats.mannwhitneyu(data_1, data_2)\n",
    "    est, mag = VD_A(data_1, data_2)\n",
    "    \n",
    "    print(\"{} vs. {}: p-value {:.1e} eff {:.2f} \".format(pair[0], pair[1], pvalue, est))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "input=\"results/finalTests\"\n",
    "\n",
    "# decomment the comment to avoid the vicunaUC_vicunaUC folder, to see closer the other results\n",
    "folders = [f.path for f in os.scandir(input) if f.is_dir()] #and f.name != \"vicunaUC_vicunaUC\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "input=\"results/finalTests\"\n",
    "\n",
    "plt.rcParams.update({'font.size': 19})\n",
    "fig = plt.figure(figsize=(11, 3))\n",
    "\n",
    "#grouped(folders, \"\", \"svg\", True, \"system_under_test\", \"max\", \"line\", save=False)\n",
    "grouped(folders, \"figures/\", \"pdf\", True, \"prompt_generator\", \"max\", \"line\", fig, save=True, mapping={'prompt_generator': 'Prompt generator', 'llama3': 'Llama 3', 'mistral': 'Mistral', 'vicuna': 'Vicuna', 'vicunaUC': 'VicunaU'})\n",
    "#folders = [f.path for f in os.scandir(input) if f.is_dir() and f.name != \"vicunaUC_vicunaUC\"]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## RQ2"
  },
  {
   "cell_type": "code",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "results=[\"results/finalTests\", \"results/rebuttal\"]\n",
    "# each config is a folder with the results of the experiments\n",
    "for results in results:\n",
    "    folders.extend([f.path for f in os.scandir(results) if f.is_dir()]) #and f.name != \"vicunaUC_vicunaUC\"])\n",
    "\n",
    "df=load_data(input=folders, criteria=\"max\", includeBaseline=True, max_samples=100)\n",
    "df[\"file\"]=df[\"file\"].replace({\"max\":\"Vanilla\", \"max_fs\": \"IE\", \"max_fs_glit\": \"IE+GL\", \"max_mem_5_fs_glit\":\"IE+SE+GL\", \"baseline\": \"RS\", \"JailbreakPrompts-mistral\": \"JailbreakPrompts\", \"JailbreakPrompts-vicunaUC\": \"JailbreakPrompts\", \"JailbreakPrompts-vicuna\": \"JailbreakPrompts\",\"JailbreakPrompts-llama3\": \"JailbreakPrompts\"})\n",
    "#df[\"file\"]=pd.Categorical(df[\"file\"], categories=[\"RS\", \"Vanilla\", \"IE\", \"IE+GL\", \"IE+SE+GL\"], ordered=True)\n",
    "\n",
    "# create a row of plots, one for each system under test\n",
    "#fig, ax= plt.subplots(1, len(suts), figsize=(7*len(suts), 7))#, sharey=True)\n",
    "\n",
    "# add new column to the dataframe, in which the delta are added\n",
    "df[\"total_delta\"]=df[\"delta_time_evaluation\"]+df[\"delta_time_generation\"]+df[\"delta_time_response\"]\n",
    "\n",
    "# melt the dataset to have the delta times in a single column and the type of delta time in another column\n",
    "dfTime=pd.melt(df, id_vars=[\"iteration\", \"score\", \"system_under_test\", \"prompt_generator\", \"file\", \"numberIterations\", \"total_delta\"], value_vars=[\"delta_time_evaluation\", \"delta_time_generation\", \"delta_time_response\"], var_name=\"delta_type\", value_name=\"delta_time\")\n",
    "\n",
    "dfTime[\"delta_type\"]=dfTime[\"delta_type\"].replace({\"delta_time_evaluation\":\"Oracle\", \"delta_time_generation\": \"PG\", \"delta_time_response\": \"SUT\" })\n",
    "\n",
    "plt.rcParams.update({'font.size': 19})\n",
    "\n",
    "# remove the delta_time=0\n",
    "dfTime=dfTime[dfTime[\"delta_time\"]>0]\n",
    "# for each plot, the x-axis is the iteration, the y axis is the delta time\n",
    "for i, sut in enumerate(suts):\n",
    "    # for each sut plot the delta time at each iteration\n",
    "    data=dfTime[dfTime[\"system_under_test\"]==sut]\n",
    "    # drop the columns that have -1 as value for any delta time (old version of the code, in which the delta times were not measured)\n",
    "    #data=data[(data[\"delta_time_evaluation\"]!=-1) & (data[\"delta_time_generation\"]!=-1) & (data[\"delta_time_response\"]!=-1)]\n",
    "    # drop the rows in which the delta time is -1\n",
    "    data=data[data[\"delta_time\"]!=-1]\n",
    "\n",
    "    fig = plt.figure(figsize=(3.5, 5))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # plot the delta time for each configuration, using hue to differentiate the type of delta time\n",
    "    # sns.boxplot(x=\"file\", y=\"delta_time\", hue=\"delta_type\", data=data, ax=ax, palette=\"pastel\")\n",
    "    sns.boxplot(x=\"file\", y=\"delta_time\", hue=\"delta_type\", data=data, ax=ax)\n",
    "    #sns.boxplot(x=\"file\", y=\"delta_time\", data=data, ax=ax[i])\n",
    "    ax.set(ylim=(0.1, 300))\n",
    "    plt.yscale('log')\n",
    "    # ax.tick_params(axis='x', rotation=45)\n",
    "    ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=90, ha='right')\n",
    "    # ax.set_title(\"SUT: \"+sut)\n",
    "    ax.set_ylabel(\"Time [s]\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.legend(ncol=1, fontsize=\"12\", loc=\"upper left\")\n",
    "    plt.savefig(\"./figures/rq2_{}.pdf\".format(sut), bbox_inches=\"tight\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "configurations = ['RS', 'Vanilla', 'IE', 'IE+GL', 'IE+SE+GL']\n",
    "\n",
    "for i, sut in enumerate(suts):\n",
    "    data = dfTime[dfTime[\"system_under_test\"] == sut]\n",
    "    data = data[data[\"delta_time\"] > 0]\n",
    "    \n",
    "    evotox_times = []\n",
    "    rs_time = 0\n",
    "    \n",
    "    for conf in configurations:\n",
    "        data_2 = data[data[\"file\"]==conf]\n",
    "\n",
    "        oracle_times = data_2[data_2[\"delta_type\"]==\"Oracle\"][\"delta_time\"].to_list()\n",
    "        pg_times = data_2[data_2[\"delta_type\"]==\"PG\"][\"delta_time\"].to_list()\n",
    "        sut_times = data_2[data_2[\"delta_type\"]==\"SUT\"][\"delta_time\"].to_list()\n",
    "                \n",
    "        if conf == 'RS':\n",
    "            min_len = min(len(oracle_times), len(sut_times))\n",
    "        else:\n",
    "            min_len = min(len(oracle_times), len(pg_times), len(sut_times))\n",
    "        \n",
    "        avg_time = 0\n",
    "        for j, time in enumerate(oracle_times[:min_len]):\n",
    "            if conf == 'RS':\n",
    "                avg_time = (avg_time*j + time + sut_times[j]) / (j + 1)\n",
    "            else:\n",
    "                avg_time = (avg_time*j + time + pg_times[j] + sut_times[j]) / (j + 1)\n",
    "            \n",
    "        if conf == 'RS':\n",
    "            rs_time = avg_time\n",
    "        else:\n",
    "            evotox_times.append(avg_time)\n",
    "            \n",
    "    print(\"{}: RS {}, EvoTox {}, +{:.2f}%\".format(sut, rs_time, sum(evotox_times)/len(evotox_times), (sum(evotox_times)/len(evotox_times)-rs_time)/rs_time*100))\n",
    "        \n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## RQ3"
  },
  {
   "cell_type": "code",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# using the function load_data, the data is loaded in a pandas dataframe with the columns: iteration, score, system_generator, prompt_generator, delta_time_evaluation, delta_time_generation, delta_time_response, numberIterations\n",
    "results=\"results/finalTests\"\n",
    "# each config is a folder with the results of the experiments\n",
    "folders = [f.path for f in os.scandir(results) if f.is_dir()] #and f.name != \"vicunaUC_vicunaUC\"]\n",
    "\n",
    "df=load_data(input=folders, criteria=\"max\", includeBaseline=True)\n",
    "df=df[(df[\"category\"]!=\"initial\")]\n",
    "\n",
    "category_mapping = {'racist': 'Racist', 'homophobic': 'Homophobic', 'sexist': 'Sexist', 'toxic': 'Toxic', 'insulting': 'Insulting'}\n",
    "df[\"category\"] = [category_mapping[cat] for cat in df[\"category\"]]\n",
    "\n",
    "# list of systems under test\n",
    "suts=df[\"system_under_test\"].unique()[:-1]\n",
    "\n",
    "dfCategories=pd.DataFrame()\n",
    "for i, sut in enumerate(suts):\n",
    "    #for each sut plot the categories\n",
    "    data=df[(df[\"system_under_test\"]==sut) & (df[\"system_under_test\"]!=\"vicunaU\")][\"category\"]\n",
    "    # sum by category\n",
    "    category_df=pd.DataFrame(data.value_counts())\n",
    "    # normalise the dataframe\n",
    "    category_df=category_df/category_df.sum()\n",
    "\n",
    "    # add the results of the sut as a column\n",
    "    dfCategories[sut]=category_df\n",
    "    # plot the categories\n",
    "    #plotCategories(categories_df=category_df, output=\"\", extension=\"\",verbose=True,legend=[sut], save=False, title=\"Normalised categories distribution for System Under Test: \"+sut)\n",
    "\n",
    "dfCategories = dfCategories.rename(columns={'llama3': 'Llama 3', 'mistral': 'Mistral', 'vicuna': 'Vicuna', 'vicunaUC': 'VicunaU'})\n",
    "\n",
    "dfCategories.plot(kind=\"bar\", stacked=False, edgecolor=\"black\", figsize=(12, 5),  color=sns.color_palette())  # color=['#44729d', '#d48640', '#539045'])\n",
    "\n",
    "# set size of the plot\n",
    "# plt.rcParams.update({'font.size': 19})\n",
    "\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# plot the legend inside the plot\n",
    "#plt.legend(['Llama 3', 'Mistral', 'Vicuna', 'VicunaU'], title='SUT', loc=\"upper center\", fontsize=\"12\")\n",
    "plt.legend(['Llama 3', 'Mistral', 'Vicuna'], title='SUT', loc=\"upper right\")  # , fontsize=\"12\")\n",
    "plt.ylim(0, 0.35)\n",
    "# set the right padding to show the legend\n",
    "# plt.subplots_adjust(right=0.95, left=0.075, top=0.9)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(\"./figures/rq3.pdf\", bbox_inches=\"tight\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RQ 4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Perplexity"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tools.perplexityPlotter import sortValues, plotPerplexity, getPerplexityFiles\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "perplexityFiles = getPerplexityFiles(\"results/finalTests\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def dataClean(data):\n",
    "    # in the model column replace \"eleutherai\" with \"EleutherAI\", my mistake\n",
    "    data['Model'] = data['Model'].replace('eleutherai/pythia-12b', 'EleutherAI/pythia-12b')\n",
    "\n",
    "    # replace dots with underscores\n",
    "    data['Model'] = data['Model'].str.replace('.', '_')\n",
    "\n",
    "    # replace - in the handle with _\n",
    "    data['Handle'] = data['Handle'].str.replace('-', '_')\n",
    "\n",
    "    # lower case for all handles\n",
    "    data['Handle'] = data['Handle'].str.lower()\n",
    "\n",
    "    # remove the part after _ in the handles that start with jailbreakprompts\n",
    "    data['Handle'] = data['Handle'].apply(\n",
    "    lambda x: x.split('_')[0] if x.startswith('jailbreakprompts') else x\n",
    "    )\n",
    "    sut_mapping = {\n",
    "        'llama3': 'Llama3',\n",
    "        'mistral': 'Mistral',\n",
    "        'vicuna': 'Vicuna',\n",
    "        'vicunaUC': 'VicunaU',\n",
    "    }\n",
    "\n",
    "    handle_mapping = {\n",
    "        'baseline': 'RS',\n",
    "        'max': 'EvoTox Vanilla',\n",
    "        'max_fs': 'EvoTox IE',\n",
    "        'max_fs_glit': 'EvoTox IE+GL',\n",
    "        'max_mem_5_fs_glit': 'EvoTox IE+SE+GL',\n",
    "        'jailbreakprompts': 'Jailbreak',\n",
    "        \n",
    "    }\n",
    "\n",
    "    data[\"SUT\"] = data[\"SUT\"].map(sut_mapping)\n",
    "\n",
    "    data[\"Handle\"] = data[\"Handle\"].map(handle_mapping)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#organise the files into a dataframe and remove the outliers at more than 3 std\n",
    "import os\n",
    "precomputed = os.path.exists('results/finalTests/perplexity_outliersRemoved.json')\n",
    "if precomputed:\n",
    "    dfPerplexity = pd.read_json(\"results/finalTests/perplexity_outliersRemoved.json\")\n",
    "    #load csv\n",
    "    #dfPerplexity = pd.read_csv(\"results/finalTests/perplexity_outliersRemoved.csv\")\n",
    "else:\n",
    "    dfPerplexity = sortValues(perplexityFiles, deleteOutliers=3)\n",
    "    # save in perplexity_outliersRemoved.json\n",
    "    with open('results/finalTests/perplexity_outliersRemoved.json', 'w') as f:\n",
    "        f.write(dfPerplexity.to_json())\n",
    "\n",
    "\n",
    "data = dataClean(dfPerplexity)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def boxplot(data, format='pdf', pplModel=['5-gram_book_corpus_sentences_arpa'], suts=None, splitSuts=False, hue='SUT', output='perplexity_box', folder='./figures/'):\n",
    "    '''\n",
    "    '''\n",
    "    # make the first letter of the suts uppercase\n",
    "    #data['SUT'] = data['SUT'].str.capitalize()\n",
    "\n",
    "    # create a figure with as many subplots as models\n",
    "    if suts:\n",
    "        suts = [sut.str.capitalize() for sut in suts]\n",
    "        # if the suts are given, the number of models is the number of suts\n",
    "        modelNumber = len(suts)\n",
    "        data = data[data['SUT'].isin(suts)]\n",
    "    elif suts is None and splitSuts:\n",
    "        # if the suts are not given, the number of models is the number of unique suts that have some data\n",
    "        suts=[]\n",
    "        for sut in data['SUT'].unique():\n",
    "            if len(data[(data['SUT']==sut) & (data['Handle']!='RS') & (data['Handle']!='Jailbreak')])>0 and sut not in suts:\n",
    "                suts.append(sut)\n",
    "        modelNumber = len(suts)\n",
    "        # suts=data['SUT'].unique()\n",
    "        # modelNumber = len(data['SUT'].unique())\n",
    "    else:\n",
    "        suts=[]\n",
    "        for sut in data['SUT'].unique():\n",
    "            if len(data[(data['SUT']==sut) & (data['Handle']!='RS') & (data['Handle']!='Jailbreak')])>0 and sut not in suts:\n",
    "                suts.append(sut)\n",
    "        # if the suts are not given and it does not have to be split, the number of models is 1\n",
    "        modelNumber = 1\n",
    "        \n",
    "    # select the chosen pplModel, if it is not given, use all the models\n",
    "    if pplModel:\n",
    "        data = data[data['Model'].isin(pplModel)]\n",
    "\n",
    "        \n",
    "    \n",
    "    # extract the baseline and jailbreak and remove from data\n",
    "    baseline = data[data['Handle']=='RS']\n",
    "    jailbreak = data[data['Handle']=='Jailbreak']\n",
    "    data = data[data['Handle']!='RS']\n",
    "    data = data[data['Handle']!='Jailbreak']\n",
    "    data = data[data['Iteration']!=0]\n",
    "    plt.rcParams.update({'font.size': 19})\n",
    "\n",
    "    # plot the boxplot\n",
    "    if splitSuts:\n",
    "        fig, axs = plt.subplots(1, modelNumber, figsize=(7*modelNumber, 5))\n",
    "        for i, sut in enumerate(suts):\n",
    "            sns.boxplot(x='Handle', y='Perplexity', data=data[data['SUT']==sut], ax=axs[i], showmeans=True, hue=hue)\n",
    "            axs[i].set_title(sut)\n",
    "            axs[i].set_ylabel(\"Perplexity\")\n",
    "            axs[i].set_xlabel(\"\")\n",
    "            axs[i].set_xticks(axs[i].get_xticks(), axs[i].get_xticklabels(), rotation=30, ha='right')\n",
    "            \n",
    "            # set y ticks in order to have the lowest and the maximum as ticks, rounded to the nearest 1000, pick maximum 7 ticks\n",
    "            axs[i].set_yticks(np.round(np.linspace(0, np.ceil(data['Perplexity'].max()/1000)*1000, 7), -1))\n",
    "            # set the limits of the y axis as the min and max of the data, rounded to the nearest 100 in excess\n",
    "            axs[i].set_ylim(0, np.ceil(data['Perplexity'].max()/1000)*1000)\n",
    "            \n",
    "            # plot the baseline and jailbreak as a horizontal line\n",
    "            axs[i].axhline(y=baseline['Perplexity'].mean(), color='magenta', linestyle='--', label='RS')\n",
    "            axs[i].axhline(y=jailbreak['Perplexity'].mean(), color='red', linestyle='--', label='Jailbreak')\n",
    "\n",
    "            # show the legend only for the first subplot and only for the horizontal lines\n",
    "            if i==0:\n",
    "                # remove the suts from the legend\n",
    "                handles, labels = axs[i].get_legend_handles_labels()\n",
    "                # update the legend with the horizontal lines, make the legend horizontal\n",
    "                axs[i].legend(handles[1:3], labels[1:3], loc=\"upper right\", fontsize=\"12\")\n",
    "            else:\n",
    "                # do not show the legend\n",
    "                axs[i].legend().set_visible(False)   \n",
    "    else:\n",
    "        # boxplot\n",
    "        fig = plt.figure(figsize=(12, 5))\n",
    "        ax = plt.gca()\n",
    "        # plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "        sns.boxplot(x='Handle', y='Perplexity', data=data, ax=ax, showmeans=True, hue=hue, meanprops={'marker':'^', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':'9'}\n",
    "                    , order=['EvoTox Vanilla', 'EvoTox IE', 'EvoTox IE+GL', 'EvoTox IE+SE+GL'], hue_order=['Llama3', 'Mistral', 'Vicuna', 'VicunaU'])\n",
    "        ax.set_ylabel(\"Perplexity\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=30, ha='right')\n",
    "        # set y ticks in order to have the lowest and the maximum as ticks, rounded to the nearest 1000, pick maximum 7 ticks\n",
    "        ax.set_yticks(np.round(np.linspace(0, np.ceil(data['Perplexity'].max()/1000)*1000, 7), -1))\n",
    "\n",
    "        # set the limits of the y axis as the min and max of the data, rounded to the nearest 100 in excess\n",
    "        ax.set_ylim(0, np.ceil(data['Perplexity'].max()/1000)*1000)\n",
    "        # legend upper right\n",
    "        # plot the baseline and jailbreak as a horizontal line\n",
    "        ax.axhline(y=baseline['Perplexity'].mean(), color='magenta', linestyle='-', label='RS')\n",
    "        ax.axhline(y=jailbreak['Perplexity'].mean(), color='red', linestyle='--', label='Jailbreak')\n",
    "        # update the legend with the horizontal lines, make the legend horizontal\n",
    "        ax.legend(title='SUT', ncol=3)  # loc=\"upper right\")\n",
    "\n",
    "\n",
    "    plt.savefig(folder+output+'.'+format, bbox_inches=\"tight\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "#plot the perplexity\n",
    "#plotPerplexity(dfPerplexity)\n",
    "#plotPerplexity(dfPerplexity, hue=\"Model\")\n",
    "#plotPerplexity(dfPerplexity, format='pdf',pplModels=[\"5-gram_book_corpus_sentences_arpa\"], suts=['llama3', 'vicuna', 'mistral'])\n",
    "\n",
    "provadf=dfPerplexity\n",
    "\n",
    "#plot the boxplot\n",
    "boxplot(data=provadf, splitSuts=False, output=\"rq4_perplexity_box_all\")\n",
    "boxplot(data=provadf, splitSuts=True, output=\"rq4_perplexity_box_sut\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Questionnaire"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load questions data\n",
    "questions_df = pd.read_excel('questionnaires/fluency/fluency.xlsx', sheet_name='Sheet2')\n",
    "\n",
    "questions_df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load answers data\n",
    "answers_df = pd.read_excel('questionnaires/fluency/fluency.xlsx', sheet_name='Sheet1')\n",
    "# Remove all columns with NaN values\n",
    "answers_df = answers_df.dropna(axis=1, how='all')\n",
    "\n",
    "answers_df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Search NaNs\n",
    "answers_df[answers_df.isna().any(axis=1)]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Remove row with NaN response\n",
    "answers_df = answers_df.dropna(axis=0, how='any', subset=[c for c in answers_df.columns if c.startswith('A:')])\n",
    "\n",
    "answers_df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create subjects data frame\n",
    "subjects_df = answers_df[[c for c in answers_df.columns if not c.startswith('A:')]]\n",
    "\n",
    "subjects_df "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "subjects_df.to_csv('questionnaires/fluency/fluency survey subjects.csv', index=False)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create responses data frame\n",
    "data = [\n",
    "    {\n",
    "        'System A': question_data['A category'],\n",
    "        'System B': question_data['B category'],\n",
    "        'Sentence A': question_data['A'],\n",
    "        'Sentence B': question_data['B'],\n",
    "        'Subject ID': answers_data['ID'],\n",
    "        'Opinion': answers_data[[c for c in answers_data.keys() if c.startswith('A:')][i]]\n",
    "    }\n",
    "    for i, question_data in questions_df.iterrows()\n",
    "    for j, answers_data in answers_df.iterrows()\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.to_csv('questionnaires/fluency/fluency survey.csv', index=False)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.info()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(df['System A'].unique())\n",
    "print(df['System A'].unique())\n",
    "print(df['Opinion'].unique())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 1: Normalize the Opinions\n",
    "def normalize_opinion(row):\n",
    "    if 'much more fluent than' in row['Opinion']:\n",
    "        return 'A >> B' if 'A' in row['Opinion'] else 'B >> A'\n",
    "    elif 'slightly more fluent than' in row['Opinion']:\n",
    "        return 'A > B' if 'A' in row['Opinion'] else 'B > A'\n",
    "    elif 'equally fluent' in row['Opinion']:\n",
    "        return 'A = B'\n",
    "    return None\n",
    "\n",
    "df['Normalized Opinion'] = df.apply(normalize_opinion, axis=1)\n",
    "\n",
    "# Step 2: Aggregate the Data\n",
    "pairwise_results = (\n",
    "    df.groupby(['System A', 'System B', 'Normalized Opinion'])\n",
    "    .size()\n",
    "    .reset_index(name='Count')\n",
    ")\n",
    "\n",
    "# Step 3: Restructure Data for Visualization\n",
    "pivot_results = pairwise_results.pivot_table(\n",
    "    index=['System A', 'System B'], \n",
    "    columns='Normalized Opinion', \n",
    "    values='Count', \n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Melt for Seaborn compatibility\n",
    "melted_results = pd.melt(\n",
    "    pivot_results, \n",
    "    id_vars=['System A', 'System B'], \n",
    "    value_vars=pivot_results.columns[2:], \n",
    "    var_name='Opinion', \n",
    "    value_name='Count'\n",
    ")\n",
    "\n",
    "# Step 4: Plot the Results\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=melted_results, \n",
    "    x='System A', \n",
    "    y='Count', \n",
    "    hue='Opinion', \n",
    "    dodge=True,\n",
    "    hue_order=['A = B', 'A > B', 'A >> B'],\n",
    "    order=['seed', 'evolved', 'jailbreak']\n",
    ")\n",
    "\n",
    "plt.title('System Preferences (A vs. rest)')\n",
    "plt.xlabel('System A')\n",
    "plt.ylabel('Count of Opinions')\n",
    "plt.legend(title='Opinion')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('./figures/rq4_system_preferences.pdf', bbox_inches=\"tight\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "\n",
    "# Step 1: Convert Opinions into Scores\n",
    "opinion_to_score = {\n",
    "    'A much more fluent than B': 1,\n",
    "    'A slightly more fluent than B': 0.5,\n",
    "    'A and B equally fluent': 0,\n",
    "    'B slightly more fluent than A': -0.5,\n",
    "    'B much more fluent than A': -1,\n",
    "}\n",
    "\n",
    "df['Score'] = df['Opinion'].map(opinion_to_score)\n",
    "\n",
    "# Step 2: Compare Systems in Pairs\n",
    "# Create a list of unique system pairs\n",
    "system_pairs = [('evolved', 'seed'), ('evolved', 'jailbreak'), ('seed', 'jailbreak')]\n",
    "\n",
    "results = []\n",
    "\n",
    "for sys1, sys2 in system_pairs:\n",
    "    # Filter data for the current pair (both orders: A/B and B/A)\n",
    "    pair_df = df[((df['System A'] == sys1) & (df['System B'] == sys2)) | \n",
    "                 ((df['System A'] == sys2) & (df['System B'] == sys1))].copy()\n",
    "    \n",
    "    # Adjust scores so that comparisons are always in the order (sys1 - sys2)\n",
    "    pair_df['Adjusted Score'] = pair_df.apply(\n",
    "        lambda row: row['Score'] if row['System A'] == sys1 else -row['Score'], axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_score = pair_df['Adjusted Score'].mean()\n",
    "    std_score = pair_df['Adjusted Score'].std()\n",
    "    count = pair_df.shape[0]\n",
    "    \n",
    "    # Perform paired t-test\n",
    "    t_stat, p_value = ttest_rel(pair_df['Adjusted Score'], [0] * len(pair_df))\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'System 1': sys1,\n",
    "        'System 2': sys2,\n",
    "        'Mean Score': mean_score,\n",
    "        'Std Dev': std_score,\n",
    "        'Count': count,\n",
    "        't-statistic': t_stat,\n",
    "        'p-value': p_value\n",
    "    })\n",
    "\n",
    "# Step 3: Create Results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display Pairwise Results\n",
    "print('Pairwise Comparisons Results:')\n",
    "print(results_df)\n",
    "\n",
    "# Step 4: Visualize the Results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "sns.barplot(\n",
    "    data=results_df,\n",
    "    x='System 1',\n",
    "    y='Mean Score',\n",
    "    hue='System 2',\n",
    "    palette='viridis',\n",
    "    dodge=True\n",
    ")\n",
    "\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.title('Pairwise Comparison of Systems (Mean Preference Score)')\n",
    "plt.ylabel('Mean Preference Score')\n",
    "plt.xlabel('System A')\n",
    "plt.legend(title='Compared to System A')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('./figures/rq4_pairwise_system_preferences_v1.pdf', bbox_inches=\"tight\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results_df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 1: Convert Opinions into Scores\n",
    "opinion_to_score = {\n",
    "    'A much more fluent than B': 1,\n",
    "    'A slightly more fluent than B': 0.5,\n",
    "    'A and B equally fluent': 0,\n",
    "    'B slightly more fluent than A': -0.5,\n",
    "    'B much more fluent than A': -1\n",
    "}\n",
    "\n",
    "df['Score'] = df['Opinion'].map(opinion_to_score)\n",
    "\n",
    "# Step 2: Compare Systems in Pairs\n",
    "# Create a list of unique system pairs\n",
    "systems = df['System A'].unique()\n",
    "pairwise_scores = pd.DataFrame(index=systems, columns=systems, data=0.0)\n",
    "\n",
    "for sys1 in systems:\n",
    "    for sys2 in systems:\n",
    "        if sys1 == sys2:\n",
    "            pairwise_scores.loc[sys1, sys2] = 0  # Diagonal is 0 by definition\n",
    "        else:\n",
    "            # Filter data for this pair (both A/B and B/A)\n",
    "            pair_df = df[((df['System A'] == sys1) & (df['System B'] == sys2)) | \n",
    "                         ((df['System A'] == sys2) & (df['System B'] == sys1))].copy()\n",
    "            \n",
    "            # Adjust scores so that comparisons are always (sys1 - sys2)\n",
    "            pair_df['Adjusted Score'] = pair_df.apply(\n",
    "                lambda row: row['Score'] if row['System A'] == sys1 else -row['Score'], axis=1\n",
    "            )\n",
    "            \n",
    "            # Mean score for the pair\n",
    "            mean_score = pair_df['Adjusted Score'].mean()\n",
    "            pairwise_scores.loc[sys1, sys2] = mean_score\n",
    "\n",
    "# Step 3: Plot Heatmap of Pairwise Comparisons\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    pairwise_scores.astype(float), \n",
    "    annot=True, \n",
    "    cmap='RdBu_r', \n",
    "    center=0, \n",
    "    linewidths=0.5,\n",
    "    fmt='.2f',\n",
    "    xticklabels=['seed', 'evolved', 'jailbreak'],\n",
    "    yticklabels=['seed', 'evolved', 'jailbreak'],\n",
    "    vmin=-1, \n",
    "    vmax=1\n",
    ")\n",
    "\n",
    "plt.title('Pairwise Comparison Matrix (Mean Preference Scores)')\n",
    "plt.xlabel('System B')\n",
    "plt.ylabel('System A')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('./figures/rq4_pairwise_system_preferences_v2.pdf', bbox_inches=\"tight\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "\n",
    "# Step 3: Normalize Opinions - Map them to categories\n",
    "opinion_mapping = {\n",
    "    \"A much more fluent than B\": 0,\n",
    "    \"A slightly more fluent than B\": 1,\n",
    "    \"A and B equally fluent\": 2,\n",
    "    \"B slightly more fluent than A\": 3,\n",
    "    \"B much more fluent than A\": 4,\n",
    "}\n",
    "opinion_mapping_compact = {\n",
    "    \"A much more fluent than B\": 0,\n",
    "    \"A slightly more fluent than B\": 0,\n",
    "    \"A and B equally fluent\": 1,\n",
    "    \"B slightly more fluent than A\": 2,\n",
    "    \"B much more fluent than A\": 2,\n",
    "}\n",
    "df[\"Opinion Category\"] = df[\"Opinion\"].map(opinion_mapping)\n",
    "df[\"Opinion Category (compact)\"] = df[\"Opinion\"].map(opinion_mapping_compact)\n",
    "\n",
    "# Step 4:\n",
    "system_pairs = [('evolved', 'seed'), ('evolved', 'jailbreak'), ('seed', 'jailbreak')]\n",
    "\n",
    "# Step 5:\n",
    "for sys1, sys2 in system_pairs:\n",
    "    # Filter data for the current pair (both orders: A/B and B/A)\n",
    "    pair_df = df[((df['System A'] == sys1) & (df['System B'] == sys2)) | \n",
    "                 ((df['System A'] == sys2) & (df['System B'] == sys1))].copy()\n",
    "    \n",
    "    for n_categories, opinion_col in zip((5, 3), (\"Opinion Category\", \"Opinion Category (compact)\")):\n",
    "        # Group data by unique items (sentence pairs) and count opinions\n",
    "        rating_counts = pair_df.groupby([\"Sentence A\", \"Sentence B\"])[opinion_col].value_counts()\n",
    "        \n",
    "        rating_matrix = (\n",
    "            rating_counts\n",
    "            .unstack(fill_value=0)  # Convert to wide format\n",
    "            .reindex(columns=[*range(n_categories)], fill_value=0)  # Ensure all categories are included\n",
    "        )\n",
    "        \n",
    "        kappa_value = fleiss_kappa(rating_matrix.values, method='fleiss')\n",
    "        \n",
    "        print(f\"Fleiss' Kappa for System Pair ({sys1}, {sys2}) with {n_categories} categories: {kappa_value:4f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## RQ5"
  },
  {
   "cell_type": "code",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "both = True\n",
    "\n",
    "if both:\n",
    "    csv_file_name = 'questionnaires/toxicity/toxic survey - {}.csv'\n",
    "    csv_files = [csv_file_name.format(v) for v in ['v1', 'v2']]\n",
    "else:\n",
    "    VERSION = 'v2'\n",
    "    csv_files = ['questionnaire/toxicity/toxic survey - {}.csv'.format(VERSION)]\n",
    "\n",
    "text_to_likert = {'not toxic': 0, 'slightly toxic': 1, 'moderately toxic': 2, 'very toxic': 3, 'extremely toxic': 4}\n",
    "text_to_gen = {'female': 'F', 'male': 'M'}\n",
    "\n",
    "entries = []\n",
    "def_keys = ['role', 'experience', 'gender', 'age']\n",
    "\n",
    "def map_value(v):\n",
    "    if v in text_to_likert:\n",
    "        return text_to_likert[v]\n",
    "    elif v in text_to_gen:\n",
    "        return text_to_gen[v]\n",
    "    elif len(v) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            return int(v)\n",
    "        except ValueError:\n",
    "            return v\n",
    "\n",
    "init_or_evolved: Dict[str, str] = dict()        \n",
    "        \n",
    "for n_f, csv_file in enumerate(csv_files):\n",
    "    q_indexes = ['Q{}'.format(30*n_f+i+1) for i in range(30)]\n",
    "    keys = def_keys + q_indexes\n",
    "    with open(csv_file) as f:\n",
    "        reader = csv.reader(f)\n",
    "        \n",
    "        for i, row in enumerate(reader):\n",
    "            # ignores headers, first participant to v1, and rows summing initial/evolved.\n",
    "            if i==0 or (n_f == 0 and i==2) or (i!=1 and row[0]==''): \n",
    "                continue\n",
    "            # processes the row where each question is labeled as initial/evolved.\n",
    "            elif i==1:\n",
    "                for q_i, q in enumerate(row[4:]):\n",
    "                    init_or_evolved[q_indexes[q_i]] = q\n",
    "                continue\n",
    "                \n",
    "            # processes participants' evaluations.    \n",
    "            entries.append({keys[j]: map_value(col) for j, col in enumerate(row)}) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Stats\n",
    "\n",
    "print('Participants: {}\\n'.format(len(entries)))\n",
    "\n",
    "males = [e for e in entries if e['gender']=='M']\n",
    "females = [e for e in entries if e['gender']=='F']\n",
    "\n",
    "print('Males: {:.3f}%\\nFemales: {:.3f}%\\n'.format(len(males)/len(entries)*100, len(females)/len(entries)*100))\n",
    "\n",
    "roles = set([e['role'] for e in entries])\n",
    "for role in roles:\n",
    "    pop = [e for e in entries if e['role']==role]\n",
    "    print('{}: {:.3f}%'.format(role, len(pop)/len(entries)*100))\n",
    "\n",
    "print('\\n')\n",
    "age_bins = [(0, 20), (20, 25), (25, 30), (30, 35), (35, 40), (40, 50), (50, 100)]\n",
    "for b in age_bins:\n",
    "    pop = [e for e in entries if b[0]<e['age']<=b[1]]\n",
    "    print('Age [{}, {}]: {:.3f}%'.format(b[0], b[1], len(pop)/len(entries)*100))\n",
    "    \n",
    "print('\\n')\n",
    "age_bins = [(0, 2), (2, 5), (5, 10), (10, 15), (15, 20), (20, 25), (25, 30), (30, 100)]\n",
    "for b in age_bins:\n",
    "    pop = [e for e in entries if b[0]<e['experience']<=b[1]]\n",
    "    print('Exp. [{}, {}]: {:.3f}%'.format(b[0], b[1], len(pop)/len(entries)*100))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data = pd.DataFrame({\n",
    "    'question': [f'Q{i+1}' for i in range(60) for _ in [e for e in entries if f'Q{i+1}' in e and e[f'Q{i+1}'] is not None]],\n",
    "    'score': [e[f'Q{i+1}'] for i in range(60) for e in entries if f'Q{i+1}' in e and e[f'Q{i+1}'] is not None]\n",
    "})\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig = plt.figure(figsize=(25, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "# sns.boxplot(x='label', y='score', data=data, hue='label', ax=ax, palette='pastel')\n",
    "sns.boxplot(x='question', y='score', data=data, hue='question', ax=ax) # ,  showmeans=True, meanprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':'6'})\n",
    "ax.set_ylabel(\"Toxicity opinion score\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set(ylim=(-0.1, 4.1))\n",
    "ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=90, ha='left')\n",
    "plt.savefig(\"./figures/rq5_qwise.pdf\", bbox_inches=\"tight\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Inter-rater Agreement Index\n",
    "\n",
    "questions = ['Q{}'.format(i+1) for i in range(60)]\n",
    "evaluations = [[e[q] for e in entries if q in e and e[q] is not None] for q in questions]\n",
    "\n",
    "# Questions 1-30 (v1, all participants)\n",
    "\n",
    "data, categories = st.aggregate_raters(np.array(evaluations[:30]), n_cat=5)\n",
    "fleiss_k = st.fleiss_kappa(data, method='fleiss')\n",
    "print('(V1) Questions 1-30: {}'.format(fleiss_k))\n",
    "\n",
    "# Questions 30-60 (v2, all participants)\n",
    "\n",
    "data, categories = st.aggregate_raters(np.array(evaluations[30:]), n_cat=5)\n",
    "fleiss_k = st.fleiss_kappa(data, method='fleiss')\n",
    "print('(V2) Questions 31-60: {}'.format(fleiss_k))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# Avg. (among participants) Score distribution broken down by responses to initial/evolved prompts.\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "labels = ['initial', 'evolved']\n",
    "labels_mapping = dict(zip(labels, ['Seed', 'Evolved']))\n",
    "questions = ['Q{}'.format(i+1) for i in range(60)]\n",
    "\n",
    "scores: Dict[str, List[float]] = dict()\n",
    "\n",
    "for question in questions:\n",
    "    scores_by_q = [e[question] for e in entries if question in e]\n",
    "    if init_or_evolved[question] == labels[0]:\n",
    "        if labels[0] in scores:\n",
    "            scores[labels[0]].append(sum(scores_by_q)/len(scores_by_q))\n",
    "        else:\n",
    "            scores[labels[0]] = [sum(scores_by_q)/len(scores_by_q)]\n",
    "    elif init_or_evolved[question] == labels[1]:\n",
    "        if labels[1] in scores:\n",
    "            scores[labels[1]].append(sum(scores_by_q)/len(scores_by_q))\n",
    "        else:\n",
    "            scores[labels[1]] = [sum(scores_by_q)/len(scores_by_q)]\n",
    "        \n",
    "print(len(scores[labels[0]]))        \n",
    "print(len(scores[labels[1]]))        \n",
    "        \n",
    "#plt.boxplot([scores[labels[0]], scores[labels[1]]], labels=labels)  \n",
    "\n",
    "# Shift the scores by 1\n",
    "shifted_scores_label_0 = [score + 1 for score in scores[labels[0]]]\n",
    "shifted_scores_label_1 = [score + 1 for score in scores[labels[1]]]\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'score': shifted_scores_label_0 + shifted_scores_label_1,\n",
    "    'label': [labels_mapping[labels[0]]] * len(shifted_scores_label_0) + [labels_mapping[labels[1]]] * len(shifted_scores_label_1)\n",
    "})\n",
    "\n",
    "plt.rcParams.update({'font.size': 19})\n",
    "fig = plt.figure(figsize=(9, 3))\n",
    "ax = plt.gca()\n",
    "\n",
    "# sns.boxplot(x='label', y='score', data=data, hue='label', ax=ax, palette='pastel')\n",
    "sns.boxplot(x='score', y='label', data=data, hue='label', ax=ax,  showmeans=True, meanprops={'marker':'^', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':'9'})\n",
    "\n",
    "ax.set(xlim=(0.9, 5.1))\n",
    "# ax.tick_params(axis='x', rotation=35)\n",
    "# ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=30, ha='right')\n",
    "#ax.set_title(\"SUT: \"+sut)\n",
    "# ax.set_ylabel(\"5-point Likert scale\")\n",
    "ax.set_xlabel(\"Toxicity opinion score\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.yaxis.set_major_locator(MultipleLocator(1))\n",
    "plt.savefig(\"./figures/rq5.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "stat, pvalue = st.stats.mannwhitneyu(scores[labels[1]], scores[labels[0]])\n",
    "estimate, magnitude = VD_A(scores[labels[1]], scores[labels[0]])\n",
    "print(\"pval: {}, A_AB: {}, magnitude: {}\".format(pvalue, estimate, magnitude))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
