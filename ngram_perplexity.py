import os
import sys
from argparse import ArgumentParser, Namespace
import logging

import re

import json
from kenlm import Model

from joblib import Parallel, delayed, parallel_backend

from nltk.tokenize import word_tokenize

from evolutionary import Archive

from typing import Optional, Tuple, Pattern, List, Dict


KENLM_MODEL_NAME_REGEX: Pattern[str] = re.compile(r'(\d+)-gram\.(\w+)\.arpa')


def parse_model_path(model_path: str) -> Optional[Tuple[int, str]]:
    model_name = os.path.basename(model_path)
    match = KENLM_MODEL_NAME_REGEX.match(model_name)
    if match is not None:
        order, corpus = match.groups()
        order = int(order)
        return order, corpus
    else:
        return None


def score_document(lm: Model, document: str) -> float:
    return lm.perplexity(' '.join(word_tokenize(document)).lower())


def compute_perplexity(lm: Model, data_path: str, model_path: Optional[str] = None):
    # Build results path
    model_name: str = os.path.basename(model_path) if model_path is not None else 'ngram'
    file_name: str = os.path.basename(data_path)
    dir_path: str = os.path.dirname(data_path)
    for d in ['perplexity', model_name.replace('.', '_')]:
        dir_path = os.path.join(dir_path, d)
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
    results_path: str = os.path.join(dir_path, f'ppl_{file_name}')
    # Check if results have already been computed
    if os.path.exists(results_path):
        logging.info(f'Results already computed and dumped at `{results_path}`, skipping computation')

        return
    # Load prompt data
    logging.info(f'Loading prompt data from `{data_path}`')
    with open(data_path) as f:
        data: Dict = json.load(f)
    logging.info(f"Data loaded")
    # Iterate over prompts
    logging.info("Computing perplexity over data set")
    with parallel_backend('threading'):
        ppl: List[float] = Parallel(verbose=2)(
            delayed(score_document)(lm, prompt)
            for run in data['runs']
            for prompt in (
                run['initial'][
                    'prompt_from_dataset' if 'prompt_from_dataset' in run['initial'] else 'promptFromDataset'
                ],
                *(taken['input_prompt_for_generation'] for taken in run['taken'])
            )
        )
    logging.info("Perplexity computed")
    # Create results container
    logging.info("Reordering results")
    ppl_iterator = iter(ppl)
    results = {
        'handle': os.path.splitext(file_name)[0],
        'config': Archive.from_dict(data).config.to_dict(),
        'results_file': data_path,
        'n-gram': dict(
            zip(('order', 'training_corpus'), parse_model_path(model_path))
        ) if model_path is not None else None,
        'model': model_name,
        'runs': [
            {
                'initial': {
                    'prompt_from_dataset': run['initial'][
                        'prompt_from_dataset' if 'prompt_from_dataset' in run['initial'] else 'promptFromDataset'
                    ],
                    'ppl': next(ppl_iterator),
                    'score': run['initial']['score']
                },
                'taken': [
                    {
                        'input_prompt_for_generation': taken['input_prompt_for_generation'],
                        'ppl': next(ppl_iterator),
                        'score': taken['score']
                    }
                    for taken in run['taken']
                ]
            }
            for run in data['runs']
        ]
    }
    logging.info("Results reordered")
    # Save results
    logging.info(f'Saving results at `{results_path}`')
    with open(results_path, 'w') as f:
        json.dump(results, f)
    logging.info(f'Results saved')


def main(args: Namespace):
    # Start logging info
    logging.info('Script started')
    # Get n-gram language model
    lm: Model = Model(args.model)
    logging.info(f'N-gram model ready for use')
    # Score perplexity on selected data
    compute_perplexity(lm, args.data_path, model_path=args.model)
    logging.info(f'Perplexity computed')
    # Close script info
    logging.info("Script completed successfully")

    return 0


if __name__ == "__main__":
    # Instantiate argument parser
    args_parser: ArgumentParser = ArgumentParser(
        prog='evotox_ngram_model_perplexity',
        description='Script to compute the perplexity of a given n-gram language model on a given set of prompts '
                    'generated with the evolutionary search'
    )
    # Add arguments to parser
    args_parser.add_argument(
        '--model',
        type=str,
        required=True,
        help="Path to the n-gram model to use"
    )
    args_parser.add_argument(
        '--data_path',
        type=str,
        required=True,
        help="Path to the JSON file with the data generated by the evolutionary search"
    )
    # Run experiment
    main(args_parser.parse_args(sys.argv[1:]))
